{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ce5e216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16 \n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, Dropout, Flatten, Activation, BatchNormalization, Dropout, GlobalAveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ab3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = pd.read_csv('../data/birds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7687f791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class index</th>\n",
       "      <th>filepaths</th>\n",
       "      <th>labels</th>\n",
       "      <th>data set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train/ABBOTTS BABBLER/001.jpg</td>\n",
       "      <td>ABBOTTS BABBLER</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>train/ABBOTTS BABBLER/002.jpg</td>\n",
       "      <td>ABBOTTS BABBLER</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train/ABBOTTS BABBLER/003.jpg</td>\n",
       "      <td>ABBOTTS BABBLER</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>train/ABBOTTS BABBLER/004.jpg</td>\n",
       "      <td>ABBOTTS BABBLER</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>train/ABBOTTS BABBLER/005.jpg</td>\n",
       "      <td>ABBOTTS BABBLER</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class index                      filepaths           labels data set\n",
       "0            0  train/ABBOTTS BABBLER/001.jpg  ABBOTTS BABBLER    train\n",
       "1            0  train/ABBOTTS BABBLER/002.jpg  ABBOTTS BABBLER    train\n",
       "2            0  train/ABBOTTS BABBLER/003.jpg  ABBOTTS BABBLER    train\n",
       "3            0  train/ABBOTTS BABBLER/004.jpg  ABBOTTS BABBLER    train\n",
       "4            0  train/ABBOTTS BABBLER/005.jpg  ABBOTTS BABBLER    train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28bc7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = birds[birds['data set'] == 'train']\n",
    "test = birds[birds['data set'] == 'test']\n",
    "val = birds[birds['data set'] == 'valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b75983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54652, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd83bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf24a63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56054f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 34\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3ba5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54652 validated image filenames belonging to 375 classes.\n",
      "Found 1875 validated image filenames belonging to 375 classes.\n",
      "Found 1875 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1.0/255.0, \n",
    "                           shear_range=0.1, \n",
    "                           zoom_range=0.1, \n",
    "                           horizontal_flip=True, \n",
    "                           vertical_flip = True)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe = train,\n",
    "    directory=\"../data/\",\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=\"labels\",\n",
    "    subset=\"training\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(128, 128),\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0,\n",
    "                                   shear_range=0.1, \n",
    "                                   zoom_range=0.1, \n",
    "                                   horizontal_flip=True, \n",
    "                                   vertical_flip = True)\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=val,\n",
    "    directory=\"../data/\",\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=\"labels\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(128, 128),\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    directory=\"../data/\",\n",
    "    x_col=\"filepaths\",\n",
    "    y_col=None,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    target_size=(128, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db9919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelCheckpoint callback is used to save only the best model out of all the epochs...\n",
    "checkpoint = ModelCheckpoint(filepath=\"../models/resnet.h5\", verbose=2, save_best_only=True)\n",
    "vgg_checkpoint = ModelCheckpoint(filepath=\"../models/vgg16.h5\", verbose=2, save_best_only=True)\n",
    "incepV3_checkpoint = ModelCheckpoint(filepath=\"../models/inception.h5\", verbose=2, save_best_only=True)\n",
    "xcep_checkpoint = ModelCheckpoint(filepath=\"../models/Xception.h5\", verbose=2, save_best_only=True)\n",
    "\n",
    "\n",
    "# EarlyStopping callback is used to stop the training when accuracy doesn't improve for 5 epochs...\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "callbacks = [early_stop, checkpoint]\n",
    "vgg_callbacks = [early_stop, vgg_checkpoint]\n",
    "incepV3_callbacks = [early_stop, incepV3_checkpoint]\n",
    "xcep_callbacks = [early_stop, xcep_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7e8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(history, model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (10, 5)) # fig of 1 row and 2 cols with 10x5 size...\n",
    "    # In First column of figure, plotting accuracy and val accuracy from trained model object(history)...\n",
    "    axes[0].plot(range(1, len(history.history['accuracy']) + 1), history.history['accuracy'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Accuracy') \n",
    "    axes[0].plot(range(1, len(history.history['val_accuracy']) + 1), history.history['val_accuracy'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Accuracy')\n",
    "    axes[0].set_xlabel('Epochs', fontsize = 14)\n",
    "    axes[0].set_ylabel('Accuracy',fontsize = 14)\n",
    "    axes[0].set_title(f\"{model} Accuracy Training vs Testing\", fontsize = 14)\n",
    "    axes[0].legend(loc = 'best') # Location of the legend, whereever is more empty space put legent there (loc= 'best')...\n",
    "    # In Second column of figure, plotting accuracy and val accuracy from trained model object(history)...\n",
    "    axes[1].plot(range(1, len(history.history['loss']) + 1), history.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\n",
    "    axes[1].plot(range(1, len(history.history['val_loss']) + 1), history.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\n",
    "    axes[1].set_xlabel('Epochs', fontsize = 14)\n",
    "    axes[1].set_ylabel('Loss',fontsize = 14)\n",
    "    axes[1].set_title(f\"{model} Loss Training vs Testing\", fontsize = 14)\n",
    "    axes[1].legend(loc = 'best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b50ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input of size 256x256x3... hight x width = 256 x 256, number of channels = 3...\n",
    "input = Input(name = 'img_input', shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e58e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def network(base_model, input):\n",
    "    \n",
    "#     # Removes the values in the graph(network connections) but do not delete the graph itself... helps in RAM cleaning...\n",
    "#     tf.keras.backend.clear_session() \n",
    "\n",
    "#     # Making Base model layers as non-trainable...\n",
    "#     for layer in base_model.layers:\n",
    "#       layer.trainable = False\n",
    "\n",
    "#     input_layer = base_model(input) # defining input layer...\n",
    "\n",
    "\n",
    "#     # Functional API of keras is used for this network...\n",
    "\n",
    "#     # Convolution layer with 32 filters of size 3x3 and stride = 1x1, activation function used is ReLU and \"valid\" padding to keep the input and output size same...\n",
    "#     # he normal kernel initializer is used as it performs well with non-linear activation functions like ReLU... \n",
    "#     conv = Conv2D(filters = 32, kernel_size = (7, 7), strides = (1, 1), padding = 'valid', activation = 'relu', \n",
    "#               kernel_initializer = tf.keras.initializers.he_normal())(input_layer)\n",
    "#     pool = GlobalAveragePooling2D()(conv) # the pool size is set to the input size and it outputs the average of the pool...\n",
    "#     d1 = Dense(units = 64, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal())(pool)\n",
    "#     drop1 = Dropout(0.5)(d1) # dropout was used to deactivate some of the nodes to avoid overfitting...\n",
    "#     d2 = Dense(units = 128, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal())(drop1)\n",
    "#     bn = BatchNormalization()(d2) # batch normalization was used normalize weights batch wise and in turn reduce the chances of overfitting...\n",
    "#     drop2 = Dropout(0.2)(bn)\n",
    "#     d3 = Dense(units = 64, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal())(drop2)\n",
    "#     Out = Dense(units = 375, activation = 'softmax', kernel_initializer = tf.keras.initializers.he_normal())(d3)\n",
    "#     model = tf.keras.Model(input, Out) \n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d25d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(base_model, input):\n",
    "    \n",
    "    # Removes the values in the graph(network connections) but do not delete the graph itself... helps in RAM cleaning...\n",
    "    tf.keras.backend.clear_session() \n",
    "\n",
    "    # Making Base model layers as non-trainable...\n",
    "    for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "    input_layer = base_model(input) # defining input layer...\n",
    "\n",
    "\n",
    "    # Functional API of keras is used for this network...\n",
    "\n",
    "    # Convolution layer with 32 filters of size 3x3 and stride = 1x1, activation function used is ReLU and \"valid\" padding to keep the input and output size same...\n",
    "    # he normal kernel initializer is used as it performs well with non-linear activation functions like ReLU... \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), strides = (1, 1), padding = 'valid', activation = 'relu', \n",
    "              kernel_initializer = tf.keras.initializers.he_normal()))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(units = 64, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal()))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units = 128, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 64, activation = 'relu', kernel_initializer = tf.keras.initializers.he_normal()))\n",
    "    model.add(Dense(units = 375, activation = 'softmax', kernel_initializer = tf.keras.initializers.he_normal()))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4877bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 4\n",
    "# num_val_samples = len(train) // k\n",
    "# num_epochs = 100\n",
    "# all_scores = []\n",
    "\n",
    "# for i in range(k):\n",
    "#     print(f'Processing Fold #{i}')\n",
    "#     val_data = train[i * num_val_samples: (i+1) * num_val_samples]\n",
    "#     part_x_data = np.concatenate([train[: i*num_val_samples], train[(i+1)*num_val_samples:]], axis = 0)\n",
    "    \n",
    "#     part_x_data = pd.DataFrame(part_x_data)\n",
    "#     part_x_data.columns = ['class index', 'filepaths', 'labels', 'data set']\n",
    "#     print(part_x_data.head(5))\n",
    "#     # Loading weights of the ResNet101 pre-trained model without including top layers... imagenet is a dataset on which ResNet101 was trained...\n",
    "#     resnet_model = ResNet101(input_tensor = input, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "#     res_model = network(resnet_model, input)\n",
    "\n",
    "#     adam = Adam(learning_rate=0.001) # ADAM optimizer was used to reach the optimal weights...\n",
    "#     res_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     train_generator, valid_generator = data_generator(part_x_data, val_data)\n",
    "    \n",
    "#     # Fitting/training model...\n",
    "#     res_history = res_model.fit(\n",
    "#         train_generator, \n",
    "#         steps_per_epoch = len(train) / BATCH_SIZE, # number of steps in each epoch...\n",
    "#         callbacks=callbacks,\n",
    "#         epochs = num_epochs, \n",
    "#         validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bca71f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.0036 - accuracy: 0.0034\n",
      "Epoch 00001: val_loss improved from inf to 5.92939, saving model to ../models\\resnet.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3415/3415 [==============================] - 727s 210ms/step - loss: 6.0036 - accuracy: 0.0034 - val_loss: 5.9294 - val_accuracy: 0.0027\n",
      "Epoch 2/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9212 - accuracy: 0.0043\n",
      "Epoch 00002: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 557s 163ms/step - loss: 5.9212 - accuracy: 0.0043 - val_loss: 5.9327 - val_accuracy: 0.0027\n",
      "Epoch 3/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9189 - accuracy: 0.0045\n",
      "Epoch 00003: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 559s 164ms/step - loss: 5.9189 - accuracy: 0.0045 - val_loss: 5.9351 - val_accuracy: 0.0027\n",
      "Epoch 4/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9183 - accuracy: 0.0045\n",
      "Epoch 00004: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 559s 164ms/step - loss: 5.9183 - accuracy: 0.0045 - val_loss: 5.9364 - val_accuracy: 0.0027\n",
      "Epoch 5/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9181 - accuracy: 0.0045\n",
      "Epoch 00005: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 581s 170ms/step - loss: 5.9181 - accuracy: 0.0045 - val_loss: 5.9371 - val_accuracy: 0.0027\n",
      "Epoch 6/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9181 - accuracy: 0.0044\n",
      "Epoch 00006: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 559s 164ms/step - loss: 5.9181 - accuracy: 0.0044 - val_loss: 5.9374 - val_accuracy: 0.0027\n",
      "Epoch 7/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9180 - accuracy: 0.0046\n",
      "Epoch 00007: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 557s 163ms/step - loss: 5.9180 - accuracy: 0.0046 - val_loss: 5.9376 - val_accuracy: 0.0027\n",
      "Epoch 8/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9180 - accuracy: 0.0044\n",
      "Epoch 00008: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 604s 177ms/step - loss: 5.9180 - accuracy: 0.0044 - val_loss: 5.9376 - val_accuracy: 0.0027\n",
      "Epoch 9/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.9180 - accuracy: 0.0046\n",
      "Epoch 00009: val_loss did not improve from 5.92939\n",
      "3415/3415 [==============================] - 601s 176ms/step - loss: 5.9180 - accuracy: 0.0046 - val_loss: 5.9375 - val_accuracy: 0.0027\n",
      "Epoch 10/100\n",
      "2248/3415 [==================>...........] - ETA: 3:17 - loss: 5.9181 - accuracy: 0.0041"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3480/3759532699.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Fitting/training model...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m res_history = res_model.fit(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# number of steps in each epoch...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\datascience\\python\\machine_learning\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading weights of the ResNet101 pre-trained model without including top layers... imagenet is a dataset on which ResNet101 was trained...\n",
    "resnet_model = ResNet101(input_tensor = input, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "res_model = network(resnet_model, input)\n",
    "\n",
    "adam = Adam(learning_rate=0.001) # ADAM optimizer was used to reach the optimal weights...\n",
    "res_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fitting/training model...\n",
    "res_history = res_model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch = len(train) / BATCH_SIZE, # number of steps in each epoch...\n",
    "    callbacks=callbacks,\n",
    "    epochs = EPOCHS, \n",
    "    validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9c5efa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3480/3563200927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplotter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_history\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ResNet101\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'res_history' is not defined"
     ]
    }
   ],
   "source": [
    "plotter(res_history, \"ResNet101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights of the vgg16 pre-trained model without including top layers... imagenet is a dataset on which vgg16 was trained...\n",
    "vgg16_model = VGG16(input_tensor = input, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "vgg_model = network(vgg16_model)\n",
    "\n",
    "adam = Adam(learning_rate=0.001) # ADAM optimizer was used to reach the optimal weights...\n",
    "vgg_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fitting/training model...\n",
    "vgg_history = vgg_model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch = len(train) / BATCH_SIZE, # number of steps in each epoch...\n",
    "    callbacks=vgg_checkpoint,\n",
    "    epochs = EPOCHS, \n",
    "    validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(vgg_history, \"VGG16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_results = vgg_model.evaluate(test_generator)\n",
    "print(f\"Accuracy of Inception v3 transfer learning model is: {vgg_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c5117f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InceptionV3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3100/1866485271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading weights of the InceptionV3 pre-trained model without including top layers... imagenet is a dataset on which InceptionV3 was trained...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minception_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"imagenet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mincepV3_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minception_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'InceptionV3' is not defined"
     ]
    }
   ],
   "source": [
    "# Loading weights of the InceptionV3 pre-trained model without including top layers... imagenet is a dataset on which InceptionV3 was trained...\n",
    "inception_model = InceptionV3(input_tensor = input, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "incepV3_model = network(inception_model, input)\n",
    "\n",
    "adam = Adam(learning_rate=0.001) # ADAM optimizer was used to reach the optimal weights...\n",
    "incepV3_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fitting/training model...\n",
    "incepV3_history = incepV3_model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch = len(train) / BATCH_SIZE, # number of steps in each epoch...\n",
    "    callbacks=incepV3_checkpoint,\n",
    "    epochs = 50, \n",
    "    validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter(incepV3_history, \"InceptionV3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e49d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "incepV3_results = incepV3_model.evaluate(test_generator)\n",
    "print(f\"Accuracy of Inception v3 transfer learning model is: {incepV3_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3ece3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = EfficientNetB0(include_top=False)\n",
    "base_model = Xception(weights = 'imagenet', include_top = False, input_shape = (256, 256, 3))\n",
    "base_model.trainable=False\n",
    "\n",
    "\n",
    "input_layer = base_model(input, training=False)\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(base_model)\n",
    "\n",
    "Xception_model.add(Flatten()) \n",
    "Xception_model.add(Activation('relu'))\n",
    "Xception_model.add(Dense(375)) \n",
    "Xception_model.add(Activation('softmax'))\n",
    "\n",
    "# pool = Flatten()(input_layer)\n",
    "# outputs = Dense(375, activation='softmax', dtype=tf.float32, name=\"Output_layer\")(pool)\n",
    "\n",
    "# Xception_model = Model(input, outputs,name=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20137939",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 37.1840 - accuracy: 0.4592\n",
      "Epoch 00001: val_loss improved from inf to 24.46029, saving model to ../models\\efficient.h5\n",
      "3415/3415 [==============================] - 579s 169ms/step - loss: 37.1840 - accuracy: 0.4592 - val_loss: 24.4603 - val_accuracy: 0.6469\n",
      "Epoch 2/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 28.0520 - accuracy: 0.6665\n",
      "Epoch 00002: val_loss improved from 24.46029 to 19.70260, saving model to ../models\\efficient.h5\n",
      "3415/3415 [==============================] - 550s 161ms/step - loss: 28.0520 - accuracy: 0.6665 - val_loss: 19.7026 - val_accuracy: 0.7493\n",
      "Epoch 3/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 24.6238 - accuracy: 0.7315\n",
      "Epoch 00003: val_loss improved from 19.70260 to 17.91088, saving model to ../models\\efficient.h5\n",
      "3415/3415 [==============================] - 543s 159ms/step - loss: 24.6238 - accuracy: 0.7315 - val_loss: 17.9109 - val_accuracy: 0.7893\n",
      "Epoch 4/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 22.1829 - accuracy: 0.7689\n",
      "Epoch 00004: val_loss did not improve from 17.91088\n",
      "3415/3415 [==============================] - 590s 173ms/step - loss: 22.1829 - accuracy: 0.7689 - val_loss: 20.2957 - val_accuracy: 0.8005\n",
      "Epoch 5/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 20.4131 - accuracy: 0.7957\n",
      "Epoch 00005: val_loss did not improve from 17.91088\n",
      "3415/3415 [==============================] - 627s 184ms/step - loss: 20.4131 - accuracy: 0.7957 - val_loss: 19.7252 - val_accuracy: 0.8133\n",
      "Epoch 6/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 17.9721 - accuracy: 0.8203\n",
      "Epoch 00006: val_loss did not improve from 17.91088\n",
      "3415/3415 [==============================] - 616s 180ms/step - loss: 17.9721 - accuracy: 0.8203 - val_loss: 20.1844 - val_accuracy: 0.8187\n",
      "Epoch 7/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 16.8969 - accuracy: 0.8339\n",
      "Epoch 00007: val_loss did not improve from 17.91088\n",
      "3415/3415 [==============================] - 602s 176ms/step - loss: 16.8969 - accuracy: 0.8339 - val_loss: 21.8012 - val_accuracy: 0.8112\n",
      "Epoch 8/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 15.7048 - accuracy: 0.8474\n",
      "Epoch 00008: val_loss did not improve from 17.91088\n",
      "3415/3415 [==============================] - 559s 164ms/step - loss: 15.7048 - accuracy: 0.8474 - val_loss: 20.0081 - val_accuracy: 0.8421\n",
      "Epoch 9/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 15.2631 - accuracy: 0.8557\n",
      "Epoch 00009: val_loss improved from 17.91088 to 17.85383, saving model to ../models\\efficient.h5\n",
      "3415/3415 [==============================] - 535s 157ms/step - loss: 15.2631 - accuracy: 0.8557 - val_loss: 17.8538 - val_accuracy: 0.8507\n",
      "Epoch 10/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 13.7290 - accuracy: 0.8683\n",
      "Epoch 00010: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 524s 153ms/step - loss: 13.7290 - accuracy: 0.8683 - val_loss: 19.7123 - val_accuracy: 0.8405\n",
      "Epoch 11/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 13.1625 - accuracy: 0.8753\n",
      "Epoch 00011: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 528s 154ms/step - loss: 13.1625 - accuracy: 0.8753 - val_loss: 19.5199 - val_accuracy: 0.8469\n",
      "Epoch 12/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 12.1685 - accuracy: 0.8836\n",
      "Epoch 00012: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 535s 157ms/step - loss: 12.1685 - accuracy: 0.8836 - val_loss: 19.9191 - val_accuracy: 0.8448\n",
      "Epoch 13/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 11.7946 - accuracy: 0.8890\n",
      "Epoch 00013: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 537s 157ms/step - loss: 11.7946 - accuracy: 0.8890 - val_loss: 22.4478 - val_accuracy: 0.8448\n",
      "Epoch 14/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 10.5771 - accuracy: 0.8984\n",
      "Epoch 00014: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 534s 156ms/step - loss: 10.5771 - accuracy: 0.8984 - val_loss: 19.9659 - val_accuracy: 0.8555\n",
      "Epoch 15/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 10.7341 - accuracy: 0.8997\n",
      "Epoch 00015: val_loss did not improve from 17.85383\n",
      "3415/3415 [==============================] - 539s 158ms/step - loss: 10.7341 - accuracy: 0.8997 - val_loss: 20.4807 - val_accuracy: 0.8608\n",
      "Epoch 16/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 10.1912 - accuracy: 0.9057\n",
      "Epoch 00016: val_loss improved from 17.85383 to 17.17629, saving model to ../models\\efficient.h5\n",
      "3415/3415 [==============================] - 577s 169ms/step - loss: 10.1912 - accuracy: 0.9057 - val_loss: 17.1763 - val_accuracy: 0.8747\n",
      "Epoch 17/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 9.5970 - accuracy: 0.9102\n",
      "Epoch 00017: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 539s 158ms/step - loss: 9.5970 - accuracy: 0.9102 - val_loss: 21.1930 - val_accuracy: 0.8571\n",
      "Epoch 18/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 9.0531 - accuracy: 0.9145\n",
      "Epoch 00018: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 535s 157ms/step - loss: 9.0531 - accuracy: 0.9145 - val_loss: 21.1458 - val_accuracy: 0.8624\n",
      "Epoch 19/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 9.1868 - accuracy: 0.9158\n",
      "Epoch 00019: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 541s 158ms/step - loss: 9.1868 - accuracy: 0.9158 - val_loss: 19.9463 - val_accuracy: 0.8773\n",
      "Epoch 20/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 8.3535 - accuracy: 0.9220\n",
      "Epoch 00020: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 534s 156ms/step - loss: 8.3535 - accuracy: 0.9220 - val_loss: 17.9647 - val_accuracy: 0.8725\n",
      "Epoch 21/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 8.3359 - accuracy: 0.9241\n",
      "Epoch 00021: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 539s 158ms/step - loss: 8.3359 - accuracy: 0.9241 - val_loss: 20.4577 - val_accuracy: 0.8624\n",
      "Epoch 22/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 8.2614 - accuracy: 0.9245\n",
      "Epoch 00022: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 742s 217ms/step - loss: 8.2614 - accuracy: 0.9245 - val_loss: 21.2837 - val_accuracy: 0.8619\n",
      "Epoch 23/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 7.3745 - accuracy: 0.9306\n",
      "Epoch 00023: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 633s 185ms/step - loss: 7.3745 - accuracy: 0.9306 - val_loss: 20.6187 - val_accuracy: 0.8667\n",
      "Epoch 24/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 7.4109 - accuracy: 0.9320\n",
      "Epoch 00024: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 541s 158ms/step - loss: 7.4109 - accuracy: 0.9320 - val_loss: 19.1450 - val_accuracy: 0.8811\n",
      "Epoch 25/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 7.1841 - accuracy: 0.9326\n",
      "Epoch 00025: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 545s 160ms/step - loss: 7.1841 - accuracy: 0.9326 - val_loss: 22.6655 - val_accuracy: 0.8811\n",
      "Epoch 26/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.8537 - accuracy: 0.9353\n",
      "Epoch 00026: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 542s 159ms/step - loss: 6.8537 - accuracy: 0.9353 - val_loss: 19.5637 - val_accuracy: 0.8816\n",
      "Epoch 27/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.6934 - accuracy: 0.9374\n",
      "Epoch 00027: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 545s 160ms/step - loss: 6.6934 - accuracy: 0.9374 - val_loss: 20.4291 - val_accuracy: 0.8805\n",
      "Epoch 28/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.7058 - accuracy: 0.9384\n",
      "Epoch 00028: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 6.7058 - accuracy: 0.9384 - val_loss: 17.2810 - val_accuracy: 0.8928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.6018 - accuracy: 0.9410\n",
      "Epoch 00029: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 543s 159ms/step - loss: 6.6018 - accuracy: 0.9410 - val_loss: 21.4448 - val_accuracy: 0.8795\n",
      "Epoch 30/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.1654 - accuracy: 0.9425\n",
      "Epoch 00030: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 6.1654 - accuracy: 0.9425 - val_loss: 20.8818 - val_accuracy: 0.8827\n",
      "Epoch 31/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.0385 - accuracy: 0.9436\n",
      "Epoch 00031: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 546s 160ms/step - loss: 6.0385 - accuracy: 0.9436 - val_loss: 19.3935 - val_accuracy: 0.8944\n",
      "Epoch 32/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 6.0955 - accuracy: 0.9443\n",
      "Epoch 00032: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 6.0955 - accuracy: 0.9443 - val_loss: 20.0899 - val_accuracy: 0.8933\n",
      "Epoch 33/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.7761 - accuracy: 0.9461\n",
      "Epoch 00033: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 544s 159ms/step - loss: 5.7761 - accuracy: 0.9461 - val_loss: 19.7719 - val_accuracy: 0.8853\n",
      "Epoch 34/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.4389 - accuracy: 0.9487\n",
      "Epoch 00034: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 5.4389 - accuracy: 0.9487 - val_loss: 17.5881 - val_accuracy: 0.8896\n",
      "Epoch 35/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.4288 - accuracy: 0.9509\n",
      "Epoch 00035: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 546s 160ms/step - loss: 5.4288 - accuracy: 0.9509 - val_loss: 21.3944 - val_accuracy: 0.8832\n",
      "Epoch 36/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.1786 - accuracy: 0.9511\n",
      "Epoch 00036: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 538s 158ms/step - loss: 5.1786 - accuracy: 0.9511 - val_loss: 19.4985 - val_accuracy: 0.8960\n",
      "Epoch 37/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.1674 - accuracy: 0.9528\n",
      "Epoch 00037: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 542s 159ms/step - loss: 5.1674 - accuracy: 0.9528 - val_loss: 20.8877 - val_accuracy: 0.8848\n",
      "Epoch 38/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.0178 - accuracy: 0.9539\n",
      "Epoch 00038: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 541s 158ms/step - loss: 5.0178 - accuracy: 0.9539 - val_loss: 17.9073 - val_accuracy: 0.8960\n",
      "Epoch 39/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 5.0554 - accuracy: 0.9539\n",
      "Epoch 00039: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 546s 160ms/step - loss: 5.0554 - accuracy: 0.9539 - val_loss: 19.7186 - val_accuracy: 0.8955\n",
      "Epoch 40/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.7551 - accuracy: 0.9559\n",
      "Epoch 00040: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 539s 158ms/step - loss: 4.7551 - accuracy: 0.9559 - val_loss: 21.8191 - val_accuracy: 0.8917\n",
      "Epoch 41/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.7003 - accuracy: 0.9560\n",
      "Epoch 00041: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 544s 159ms/step - loss: 4.7003 - accuracy: 0.9560 - val_loss: 21.6978 - val_accuracy: 0.8875\n",
      "Epoch 42/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.6067 - accuracy: 0.9579\n",
      "Epoch 00042: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 4.6067 - accuracy: 0.9579 - val_loss: 19.9379 - val_accuracy: 0.8912\n",
      "Epoch 43/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.5892 - accuracy: 0.9579\n",
      "Epoch 00043: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 545s 159ms/step - loss: 4.5892 - accuracy: 0.9579 - val_loss: 23.4276 - val_accuracy: 0.8885\n",
      "Epoch 44/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.6205 - accuracy: 0.9586\n",
      "Epoch 00044: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 540s 158ms/step - loss: 4.6205 - accuracy: 0.9586 - val_loss: 18.8911 - val_accuracy: 0.8981\n",
      "Epoch 45/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.2937 - accuracy: 0.9608\n",
      "Epoch 00045: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 544s 159ms/step - loss: 4.2937 - accuracy: 0.9608 - val_loss: 23.1306 - val_accuracy: 0.8928\n",
      "Epoch 46/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.4799 - accuracy: 0.9603\n",
      "Epoch 00046: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 542s 159ms/step - loss: 4.4799 - accuracy: 0.9603 - val_loss: 22.0243 - val_accuracy: 0.9019\n",
      "Epoch 47/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.2366 - accuracy: 0.9622\n",
      "Epoch 00047: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 546s 160ms/step - loss: 4.2366 - accuracy: 0.9622 - val_loss: 19.8513 - val_accuracy: 0.9024\n",
      "Epoch 48/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.1945 - accuracy: 0.9622\n",
      "Epoch 00048: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 544s 159ms/step - loss: 4.1945 - accuracy: 0.9622 - val_loss: 23.0258 - val_accuracy: 0.8912\n",
      "Epoch 49/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.2781 - accuracy: 0.9621\n",
      "Epoch 00049: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 575s 168ms/step - loss: 4.2781 - accuracy: 0.9621 - val_loss: 18.9505 - val_accuracy: 0.9109\n",
      "Epoch 50/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 4.0843 - accuracy: 0.9628\n",
      "Epoch 00050: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 584s 171ms/step - loss: 4.0843 - accuracy: 0.9628 - val_loss: 21.0775 - val_accuracy: 0.8917\n",
      "Epoch 51/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.8716 - accuracy: 0.9631\n",
      "Epoch 00051: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 555s 163ms/step - loss: 3.8716 - accuracy: 0.9631 - val_loss: 22.3544 - val_accuracy: 0.8981\n",
      "Epoch 52/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.8157 - accuracy: 0.9663\n",
      "Epoch 00052: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 602s 176ms/step - loss: 3.8157 - accuracy: 0.9663 - val_loss: 21.8102 - val_accuracy: 0.9077\n",
      "Epoch 53/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.7815 - accuracy: 0.9660\n",
      "Epoch 00053: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 648s 190ms/step - loss: 3.7815 - accuracy: 0.9660 - val_loss: 23.5555 - val_accuracy: 0.8912\n",
      "Epoch 54/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.7426 - accuracy: 0.9665\n",
      "Epoch 00054: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 606s 178ms/step - loss: 3.7426 - accuracy: 0.9665 - val_loss: 23.7660 - val_accuracy: 0.8939\n",
      "Epoch 55/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.7487 - accuracy: 0.9661\n",
      "Epoch 00055: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 646s 189ms/step - loss: 3.7487 - accuracy: 0.9661 - val_loss: 22.3787 - val_accuracy: 0.9061\n",
      "Epoch 56/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.8861 - accuracy: 0.9661\n",
      "Epoch 00056: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 651s 191ms/step - loss: 3.8861 - accuracy: 0.9661 - val_loss: 21.1788 - val_accuracy: 0.9061\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3416/3415 [==============================] - ETA: 0s - loss: 3.4952 - accuracy: 0.9677\n",
      "Epoch 00057: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 629s 184ms/step - loss: 3.4952 - accuracy: 0.9677 - val_loss: 23.9286 - val_accuracy: 0.8949\n",
      "Epoch 58/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.5849 - accuracy: 0.9685\n",
      "Epoch 00058: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 619s 181ms/step - loss: 3.5849 - accuracy: 0.9685 - val_loss: 25.0468 - val_accuracy: 0.8901\n",
      "Epoch 59/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.4610 - accuracy: 0.9683\n",
      "Epoch 00059: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 637s 186ms/step - loss: 3.4610 - accuracy: 0.9683 - val_loss: 20.6938 - val_accuracy: 0.9008\n",
      "Epoch 60/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.3579 - accuracy: 0.9697\n",
      "Epoch 00060: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 631s 185ms/step - loss: 3.3579 - accuracy: 0.9697 - val_loss: 20.7037 - val_accuracy: 0.9077\n",
      "Epoch 61/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.1776 - accuracy: 0.9707\n",
      "Epoch 00061: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 649s 190ms/step - loss: 3.1776 - accuracy: 0.9707 - val_loss: 21.6285 - val_accuracy: 0.8971\n",
      "Epoch 62/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.2136 - accuracy: 0.9711\n",
      "Epoch 00062: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 655s 192ms/step - loss: 3.2136 - accuracy: 0.9711 - val_loss: 21.4923 - val_accuracy: 0.9051\n",
      "Epoch 63/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.1581 - accuracy: 0.9716\n",
      "Epoch 00063: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 648s 190ms/step - loss: 3.1581 - accuracy: 0.9716 - val_loss: 21.4416 - val_accuracy: 0.9056\n",
      "Epoch 64/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.3701 - accuracy: 0.9702\n",
      "Epoch 00064: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 633s 185ms/step - loss: 3.3701 - accuracy: 0.9702 - val_loss: 22.8876 - val_accuracy: 0.8965\n",
      "Epoch 65/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.2698 - accuracy: 0.9716\n",
      "Epoch 00065: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 661s 194ms/step - loss: 3.2698 - accuracy: 0.9716 - val_loss: 21.1504 - val_accuracy: 0.9056\n",
      "Epoch 66/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.3698 - accuracy: 0.9710\n",
      "Epoch 00066: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 644s 188ms/step - loss: 3.3698 - accuracy: 0.9710 - val_loss: 24.1320 - val_accuracy: 0.8949\n",
      "Epoch 67/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.1625 - accuracy: 0.9721\n",
      "Epoch 00067: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 711s 208ms/step - loss: 3.1625 - accuracy: 0.9721 - val_loss: 22.0845 - val_accuracy: 0.9099\n",
      "Epoch 68/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.0570 - accuracy: 0.9728\n",
      "Epoch 00068: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 895s 262ms/step - loss: 3.0570 - accuracy: 0.9728 - val_loss: 23.0315 - val_accuracy: 0.9072\n",
      "Epoch 69/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 2.7403 - accuracy: 0.9747\n",
      "Epoch 00069: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 652s 191ms/step - loss: 2.7403 - accuracy: 0.9747 - val_loss: 22.4640 - val_accuracy: 0.9035\n",
      "Epoch 70/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 2.9325 - accuracy: 0.9730\n",
      "Epoch 00070: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 638s 187ms/step - loss: 2.9325 - accuracy: 0.9730 - val_loss: 22.7559 - val_accuracy: 0.9083\n",
      "Epoch 71/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 2.8917 - accuracy: 0.9745\n",
      "Epoch 00071: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 645s 189ms/step - loss: 2.8917 - accuracy: 0.9745 - val_loss: 23.0528 - val_accuracy: 0.8997\n",
      "Epoch 72/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.1199 - accuracy: 0.9727\n",
      "Epoch 00072: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 629s 184ms/step - loss: 3.1199 - accuracy: 0.9727 - val_loss: 22.8722 - val_accuracy: 0.9051\n",
      "Epoch 73/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 2.9140 - accuracy: 0.9740\n",
      "Epoch 00073: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 658s 193ms/step - loss: 2.9140 - accuracy: 0.9740 - val_loss: 25.5057 - val_accuracy: 0.9008\n",
      "Epoch 74/100\n",
      "3416/3415 [==============================] - ETA: 0s - loss: 3.0040 - accuracy: 0.9741\n",
      "Epoch 00074: val_loss did not improve from 17.17629\n",
      "3415/3415 [==============================] - 639s 187ms/step - loss: 3.0040 - accuracy: 0.9741 - val_loss: 22.5050 - val_accuracy: 0.9051\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(learning_rate=0.001) # ADAM optimizer was used to reach the optimal weights...\n",
    "Xception_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Fitting/training model...\n",
    "Xception_history = Xception_model.fit(\n",
    "    train_generator, \n",
    "    steps_per_epoch = len(train) / BATCH_SIZE, # number of steps in each epoch...\n",
    "    callbacks=eff_callbacks,\n",
    "    epochs = EPOCHS, \n",
    "    validation_data = valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5bdad4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter(Xception_history, \"Xception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14bd899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
